<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="title" content="AmbiVer: Open-Vocabulary 3D Instruction Ambiguity Detection">
  <meta name="description" content="We define Open-Vocabulary 3D Instruction Ambiguity Detection and propose AmbiVer, a framework using the Ambi3D benchmark to ensure safe Embodied AI interaction by detecting ambiguous commands.">
  <meta name="keywords" content="Embodied AI, 3D LLMs, Ambiguity Detection, Safety-critical AI, Ambi3D, AmbiVer, Vision-Language Models">
  
  <meta name="author" content="Jiayu Ding, Haoran Tang, Ge Li"> <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Peking University">
  <meta property="og:title" content="AmbiVer: Open-Vocabulary 3D Instruction Ambiguity Detection">
  <meta property="og:description" content="We define Open-Vocabulary 3D Instruction Ambiguity Detection and propose AmbiVer, a framework using the Ambi3D benchmark to ensure safe Embodied AI interaction by detecting ambiguous commands.">
  <meta property="og:url" content="https://[YOUR_DOMAIN].com/ambiver">
  <meta property="og:image" content="static/images/social_preview.png"> <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="AmbiVer: Open-Vocabulary 3D Instruction Ambiguity Detection">
  <meta name="twitter:description" content="We define Open-Vocabulary 3D Instruction Ambiguity Detection and propose AmbiVer, a framework using the Ambi3D benchmark to ensure safe Embodied AI interaction by detecting ambiguous commands.">
  <meta name="twitter:image" content="static/images/social_preview.png">

  <meta name="citation_title" content="AmbiVer: Open-Vocabulary 3D Instruction Ambiguity Detection">
  <meta name="citation_publication_date" content="2024">
  
  <title>AmbiVer: Open-Vocabulary 3D Instruction Ambiguity Detection</title>
  
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
</head>
<body>

  <main id="main-content">
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <h1 class="title is-1 publication-title">AmbiVer: Open-Vocabulary 3D Instruction Ambiguity Detection</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="mailto:jyding25@stu.pku.edu.cn">Jiayu Ding</a>,</span>
            <span class="author-block">
              <a href="mailto:hrtang@stu.pku.edu.cn">Haoran Tang</a>,</span>
            <span class="author-block">
              <a href="mailto:geli@pku.edu.cn">Ge Li</a><sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">School of Electronic and Computer Engineering, Peking University</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block" style="font-family: monospace; color: #4a4a4a;">
              {jyding25, hrtang}@stu.pku.edu.cn, geli@pku.edu.cn
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Corresponding Author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/你的用户名/AmbiVer" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-database"></i></span>
                  <span>Ambi3D Dataset</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>
    
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="text-align: center;">
         <img src="static/images/001_1.png" alt="Ambiguity Example" style="width: 100%; max-width: 800px; border-radius: 10px;">
      </div>
      
      <h2 class="subtitle has-text-centered" style="margin-top: 15px;">
        This high-stakes scenario highlights a critical safety challenge where an ambiguous instruction (<i>"Pass me the vial"</i>) forces a robot to choose between a harmless substance and a lethal one.
      </h2>
    </div>
  </div>
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define <strong>Open-Vocabulary 3D Instruction Ambiguity Detection</strong>, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. 
          </p>
          <p>
            To support this research, we build <strong>Ambi3D</strong>, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous.
          </p>
          <p>
            To address this challenge, we propose <strong>AmbiVer</strong>, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide a vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Our dataset and code will be made publicly available.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            The reliability of an agent's interaction with the physical world heavily depends on the precision of its understanding of human instructions. Current research in embodied intelligence has predominantly centered on "grounding" language in vision and subsequent "execution." While successful, this paradigm assumes instructions are unambiguous, introducing significant latent risks.
          </p>
          <p>
            For example, in a smart home context, if asked <i>"Was the stove in the kitchen turned off?"</i>, a model might check the main stovetop and answer "Yes," overlooking a portable induction cooktop still operating in a corner. This creates a false sense of security. Fundamentally, these models are designed to find the "right answer" to an input assumed to be valid, lacking an intrinsic mechanism to identify when the input itself is a "bad question."
          </p>
          <p>
            A truly reliable system must possess the ability to recognize such ambiguity and proactively seek clarification, rather than blindly guessing. To systematically address this, we introduce the task of <strong>Open-Vocabulary 3D Instruction Ambiguity Detection</strong>.
          </p>
        </div>
        
        </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">The AmbiVer Framework</h2>
        <div class="content has-text-justified">
          <p>
            We propose <strong>AmbiVer</strong> (Ambiguity Verifier), a two-stage framework that decouples scene perception from logical reasoning. 
          </p>
          <ul>
            <li><strong>Perception Stage:</strong> Converts the raw scene and instruction into a set of structured evidence, geometrically unifying potential referents across multiple views.</li>
            <li><strong>Reasoning Stage:</strong> Passes this structured evidence to a zero-shot VLM for logical adjudication.</li>
          </ul>
           <div style="text-align: center; margin-top: 20px;">
             <img src="static/images/method_diagram.jpg" alt="AmbiVer Method" style="width: 100%; max-width: 900px;">
             <p class="subtitle is-6">Overview of the AmbiVer two-stage framework.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
<pre id="bibtex-code"><code>@article{AmbiVer2024,
  title={AmbiVer: Open-Vocabulary 3D Instruction Ambiguity Detection},
  author={Ding, Jiayu and Tang, Haoran and Li, Ge},
  journal={arXiv preprint arXiv:26xx.xxxxx},
  year={2026}
}</code></pre>
    </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
  </main>
</footer>

  </body>
  </html>
